{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6154ae2",
   "metadata": {},
   "source": [
    "# Clickbait Detection using BERT\n",
    "\n",
    "Este notebook implementa la segunda parte de la práctica de detección de clickbait utilizando un modelo de lenguaje de gran tamaño (LLM) basado en BERT.\n",
    "\n",
    "> **Nota importante sobre versiones**: Este notebook está diseñado para funcionar con versiones específicas de bibliotecas: transformers==4.18.0 y torch==1.11.0. Si experimentas errores relacionados con parámetros de TrainingArguments, asegúrate de que estás utilizando estas versiones.\n",
    "\n",
    "> **Actualizaciones**: Se han implementado varias mejoras al modelo original, incluyendo diferentes esquemas de entrenamiento, modelos alternativos como RoBERTa, y técnicas de ajuste fino como LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0928dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de las bibliotecas necesarias\n",
    "!pip install torch==1.11.0 transformers==4.18.0 scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install peft==0.3.0  # Para implementar LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de las bibliotecas instaladas\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import peft\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__ if torch is not None else 'Not installed'}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__ if 'peft' in globals() else 'Not installed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655a74d",
   "metadata": {},
   "source": [
    "## 1. Importación de Bibliotecas\n",
    "\n",
    "Importamos todas las bibliotecas necesarias para el procesamiento de datos, entrenamiento del modelo y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Verificar si estamos en Google Colab y configurar GPU\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "    print(\"Ejecutando en Google Colab\")\n",
    "    \n",
    "    # Verificar GPU disponible\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No hay GPU disponible, usando CPU\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"No estamos en Colab. Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc128e0",
   "metadata": {},
   "source": [
    "## 2. Carga del Corpus\n",
    "\n",
    "Cargamos los corpus para entrenamiento, validación y prueba:\n",
    "- `TA1C_dataset_detection_train.csv` - Para entrenamiento original\n",
    "- `TA1C_dataset_detection_dev.csv` - Para validación\n",
    "- `TA1C_dataset_detection_dev_gold.csv` - Para evaluación con etiquetas de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas - ajustar según sea necesario para Google Colab\n",
    "if IN_COLAB:\n",
    "    print(\"Por favor, sube los archivos de datos:\")\n",
    "    uploaded = files.upload()  # Sube los archivos necesarios\n",
    "    \n",
    "    # Ruta de los archivos en Colab\n",
    "    train_filepath = \"TA1C_dataset_detection_train.csv\"\n",
    "    dev_filepath = \"TA1C_dataset_detection_dev.csv\"\n",
    "    dev_gold_filepath = \"TA1C_dataset_detection_dev_gold.csv\"\n",
    "    \n",
    "    # Verificar que los archivos se han subido correctamente\n",
    "    import os\n",
    "    if not all(os.path.exists(f) for f in [train_filepath, dev_filepath, dev_gold_filepath]):\n",
    "        print(\"ADVERTENCIA: No se han encontrado todos los archivos necesarios.\")\n",
    "        print(\"Por favor, asegúrate de subir los siguientes archivos:\")\n",
    "        print(\"- TA1C_dataset_detection_train.csv\")\n",
    "        print(\"- TA1C_dataset_detection_dev.csv\")\n",
    "        print(\"- TA1C_dataset_detection_dev_gold.csv\")\n",
    "else:\n",
    "    # Rutas para entorno local\n",
    "    train_filepath = \"./corpus/TA1C_dataset_detection_train.csv\"\n",
    "    dev_filepath = \"./corpus/TA1C_dataset_detection_dev.csv\"\n",
    "    dev_gold_filepath = \"./corpus/TA1C_dataset_detection_dev_gold.csv\"  # Archivo gold con etiquetas\n",
    "\n",
    "# Función para cargar y mostrar información básica de un dataset\n",
    "def load_and_display_dataset(filepath, name):\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "        print(f\"Corpus {name} cargado con {len(data)} instancias\")\n",
    "\n",
    "        # Mostrar primeras filas\n",
    "        print(f\"\\nPrimeras filas del corpus {name}:\")\n",
    "        display(data.head())\n",
    "\n",
    "        # Información básica del corpus\n",
    "        print(f\"\\nInformación del corpus {name}:\")\n",
    "        print(f\"Columnas: {data.columns.tolist()}\")\n",
    "        \n",
    "        # Verificar si existe la columna 'Tag Value'\n",
    "        if 'Tag Value' in data.columns:\n",
    "            print(f\"Distribución de clases:\")\n",
    "            print(data[\"Tag Value\"].value_counts())\n",
    "            \n",
    "        return data\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: No se encontró el archivo - {e}\")\n",
    "        print(\"\\nSolución: Asegúrate de que los archivos están en las rutas correctas.\")\n",
    "        if IN_COLAB:\n",
    "            print(\"En Google Colab, debes subir los archivos manualmente.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cargar todos los datasets\n",
    "train_data = load_and_display_dataset(train_filepath, \"de entrenamiento\")\n",
    "dev_data = load_and_display_dataset(dev_filepath, \"de desarrollo\")\n",
    "dev_gold_data = load_and_display_dataset(dev_gold_filepath, \"gold de desarrollo\")\n",
    "\n",
    "# Verificar si se cargaron correctamente\n",
    "has_gold = dev_gold_data is not None\n",
    "if not has_gold:\n",
    "    print(\"\\nADVERTENCIA: No se pudo cargar el archivo gold de desarrollo.\")\n",
    "    print(\"Algunas funcionalidades de evaluación estarán limitadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea99e9",
   "metadata": {},
   "source": [
    "## 3. Implementación de Esquemas de Entrenamiento y Evaluación\n",
    "\n",
    "Vamos a implementar los diferentes esquemas propuestos:\n",
    "\n",
    "### Esquema 1\n",
    "- 75% de TA1C_dataset_detection_train.csv para entrenamiento\n",
    "- 25% de TA1C_dataset_detection_train.csv para evaluación\n",
    "- TA1C_dataset_detection_dev_gold.csv para prueba\n",
    "\n",
    "### Esquema 2\n",
    "- 100% de TA1C_dataset_detection_train.csv para entrenamiento\n",
    "- 100% de TA1C_dataset_detection_dev_gold.csv para evaluación\n",
    "\n",
    "### Esquema 3\n",
    "- 100% de TA1C_dataset_detection_train.csv para entrenamiento\n",
    "- 50% de TA1C_dataset_detection_dev_gold.csv para evaluación\n",
    "- 50% restante de TA1C_dataset_detection_dev_gold.csv para prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preparar datos según el esquema\n",
    "def prepare_data_for_scheme(scheme):\n",
    "    \"\"\"\n",
    "    Prepara los datos según los diferentes esquemas de entrenamiento\n",
    "    \n",
    "    Args:\n",
    "        scheme: Número del esquema (1, 2 o 3)\n",
    "        \n",
    "    Returns:\n",
    "        Tupla con (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Verificar que los datasets necesarios estén cargados\n",
    "    if not train_data is not None:\n",
    "        print(\"Error: No se ha podido cargar el archivo train.csv\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "        \n",
    "    if not has_gold:\n",
    "        print(\"Error: No se ha podido cargar el archivo dev_gold.csv necesario para estos esquemas\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "    \n",
    "    if scheme == 1:\n",
    "        print(\"\\n=== Preparando datos para Esquema 1 ===\")\n",
    "        print(\"75% de train.csv para entrenamiento, 25% de train.csv para evaluación, dev_gold.csv para prueba\")\n",
    "        \n",
    "        # Extraer características y etiquetas del archivo train\n",
    "        X_train_data = train_data[\"Teaser Text\"].tolist()\n",
    "        y_train_data = train_data[\"Tag Value\"].tolist()\n",
    "        \n",
    "        # Dividir en conjuntos de entrenamiento y validación (75% - 25%)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_data, y_train_data, test_size=0.25, random_state=0, stratify=y_train_data, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Usar dev_gold.csv como conjunto de prueba\n",
    "        X_test = dev_data[\"Teaser Text\"].tolist() if dev_data is not None else []\n",
    "        y_test = dev_gold_data[\"Tag Value\"].tolist()\n",
    "            \n",
    "    elif scheme == 2:\n",
    "        print(\"\\n=== Preparando datos para Esquema 2 ===\")\n",
    "        print(\"100% de train.csv para entrenamiento, 100% de dev_gold.csv para evaluación\")\n",
    "        \n",
    "        # Usar train.csv como conjunto de entrenamiento\n",
    "        X_train = train_data[\"Teaser Text\"].tolist()\n",
    "        y_train = train_data[\"Tag Value\"].tolist()\n",
    "        \n",
    "        # Usar dev_gold.csv como conjunto de validación y prueba\n",
    "        X_val = dev_data[\"Teaser Text\"].tolist() if dev_data is not None else []\n",
    "        X_test = X_val  # Mismo conjunto para validación y prueba\n",
    "        y_val = dev_gold_data[\"Tag Value\"].tolist()\n",
    "        y_test = y_val\n",
    "            \n",
    "    elif scheme == 3:\n",
    "        print(\"\\n=== Preparando datos para Esquema 3 ===\")\n",
    "        print(\"100% de train.csv para entrenamiento, 50% de dev_gold.csv para evaluación, 50% restante para prueba\")\n",
    "        \n",
    "        # Usar train.csv como conjunto de entrenamiento\n",
    "        X_train = train_data[\"Teaser Text\"].tolist()\n",
    "        y_train = train_data[\"Tag Value\"].tolist()\n",
    "        \n",
    "        # Dividir dev_gold.csv y sus etiquetas en dos mitades\n",
    "        X_dev = dev_data[\"Teaser Text\"].tolist() if dev_data is not None else []\n",
    "        y_dev = dev_gold_data[\"Tag Value\"].tolist()\n",
    "        \n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_dev, y_dev, test_size=0.5, random_state=0, shuffle=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Esquema {scheme} no reconocido. Debe ser 1, 2 o 3\")\n",
    "        \n",
    "    # Codificar etiquetas\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    \n",
    "    if y_val is not None:\n",
    "        y_val_encoded = le.transform(y_val)\n",
    "    else:\n",
    "        y_val_encoded = None\n",
    "        \n",
    "    if y_test is not None:\n",
    "        y_test_encoded = le.transform(y_test)\n",
    "    else:\n",
    "        y_test_encoded = None\n",
    "        \n",
    "    # Guardar mapeo de etiquetas\n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(\"\\nMapeo de etiquetas:\")\n",
    "    for label, idx in label_mapping.items():\n",
    "        print(f\"  {label} -> {idx}\")\n",
    "    \n",
    "    # Mostrar estadísticas\n",
    "    print(f\"\\nConjunto de entrenamiento: {len(X_train)} instancias\")\n",
    "    print(f\"Conjunto de validación: {len(X_val)} instancias\")\n",
    "    print(f\"Conjunto de prueba: {len(X_test)} instancias\")\n",
    "    \n",
    "    # Retornar todos los conjuntos y el mapeo de etiquetas\n",
    "    return X_train, y_train_encoded, X_val, y_val_encoded, X_test, y_test_encoded, label_mapping, le\n",
    "\n",
    "# Implementar cada esquema\n",
    "data_schemes = {}\n",
    "for scheme in [1, 2, 3]:\n",
    "    data_schemes[scheme] = prepare_data_for_scheme(scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4896c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenar un modelo\n",
    "def train_model(model_name, train_dataset, val_dataset, num_labels, output_dir, \n",
    "                learning_rate=2e-5, batch_size=16, num_epochs=3, use_lora=False, lora_config=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de transformers\n",
    "    \n",
    "    Args:\n",
    "        model_name: Nombre del modelo preentrenado\n",
    "        train_dataset, val_dataset: Datasets de entrenamiento y validación\n",
    "        num_labels: Número de clases\n",
    "        output_dir: Directorio para guardar resultados\n",
    "        learning_rate, batch_size, num_epochs: Hiperparámetros\n",
    "        use_lora: Si se debe usar LoRA para el ajuste fino\n",
    "        lora_config: Configuración específica para LoRA\n",
    "        \n",
    "    Returns:\n",
    "        trainer: Objeto Trainer con el modelo entrenado\n",
    "        results: Resultados de la evaluación\n",
    "    \"\"\"\n",
    "    # Determinar tipo de modelo\n",
    "    model_type = \"roberta\" if \"roberta\" in model_name.lower() else \"bert\"\n",
    "    \n",
    "    # Cargar modelo preentrenado según tipo\n",
    "    if model_type == \"roberta\":\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_labels\n",
    "        )\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_labels\n",
    "        )\n",
    "    \n",
    "    # Aplicar LoRA si se especifica\n",
    "    if use_lora:\n",
    "        # Configuración de LoRA predeterminada si no se proporciona\n",
    "        if lora_config is None:\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=8,  # Rango de la adaptación\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\",\n",
    "                target_modules=[\"query\", \"key\", \"value\"]  # Módulos a adaptar\n",
    "            )\n",
    "        \n",
    "        # Aplicar LoRA al modelo\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print(\"Información sobre parámetros entrenables con LoRA:\")\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    # Configurar argumentos de entrenamiento\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"steps\",  # Compatible con transformers 4.18.0\n",
    "        eval_steps=50,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        seed=0,\n",
    "        load_best_model_at_end=True,\n",
    "        save_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\"  # Deshabilitar informes a Weights & Biases\n",
    "    )\n",
    "\n",
    "    # Inicializar el entrenador\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    try:\n",
    "        print(f\"Entrenando modelo {model_name} {'con LoRA' if use_lora else 'sin LoRA'}...\")\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el entrenamiento: {str(e)}\")\n",
    "        print(\"\\nTIP: Si el error está relacionado con 'evaluation_strategy', asegúrate de estar usando transformers==4.18.0\")\n",
    "        try:\n",
    "            # Intenta modificar el argumento según la versión\n",
    "            if hasattr(TrainingArguments, \"eval_strategy\"):\n",
    "                training_args.evaluation_strategy = None\n",
    "                training_args.eval_strategy = \"steps\"\n",
    "            else:\n",
    "                training_args.eval_strategy = None\n",
    "                training_args.evaluation_strategy = \"steps\"\n",
    "            \n",
    "            # Reinicializar el entrenador con los argumentos corregidos\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "            trainer.train()\n",
    "        except Exception as e2:\n",
    "            print(f\"Error durante el segundo intento: {str(e2)}\")\n",
    "            raise\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    print(\"\\nEvaluando modelo...\")\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Resultados de la evaluación: {results}\")\n",
    "    \n",
    "    return trainer, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7198fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar experimentos con diferentes configuraciones\n",
    "def run_experiments(scheme_id, X_train, y_train, X_val, y_val, label_mapping, model_configs):\n",
    "    \"\"\"\n",
    "    Ejecuta varios experimentos con diferentes modelos y configuraciones\n",
    "    \n",
    "    Args:\n",
    "        scheme_id: ID del esquema de datos usado\n",
    "        X_train, y_train, X_val, y_val: Datos preparados\n",
    "        label_mapping: Mapeo de etiquetas\n",
    "        model_configs: Lista de configuraciones de modelos a probar\n",
    "        \n",
    "    Returns:\n",
    "        results_df: DataFrame con los resultados\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    trainers = {}\n",
    "    \n",
    "    # Obtener número de clases\n",
    "    num_labels = len(label_mapping) if label_mapping else 2\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = config['model_name']\n",
    "        use_lora = config.get('use_lora', False)\n",
    "        lora_params = config.get('lora_params', None)\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Preparando experimento con modelo: {model_name} {'con LoRA' if use_lora else 'sin LoRA'}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        # Inicializar tokenizador según tipo de modelo\n",
    "        if \"roberta\" in model_name.lower():\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        else:\n",
    "            tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Preparar datasets\n",
    "        train_dataset, val_dataset = prepare_datasets(X_train, y_train, X_val, y_val, tokenizer)\n",
    "        \n",
    "        # Configurar LoRA si es necesario\n",
    "        lora_config = None\n",
    "        if use_lora and lora_params:\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=lora_params.get('r', 8),\n",
    "                lora_alpha=lora_params.get('lora_alpha', 16),\n",
    "                lora_dropout=lora_params.get('lora_dropout', 0.1),\n",
    "                bias=lora_params.get('bias', \"none\"),\n",
    "                target_modules=lora_params.get('target_modules', [\"query\", \"key\", \"value\"])\n",
    "            )\n",
    "        \n",
    "        # Nombre único para el experimento\n",
    "        exp_name = f\"{model_name.split('/')[-1]}_{config.get('name', '')}\"\n",
    "        if use_lora:\n",
    "            exp_name += \"_lora\"\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nEntrenando modelo: {exp_name}\")\n",
    "            \n",
    "            # Entrenar modelo\n",
    "            trainer, eval_results = train_model(\n",
    "                model_name=model_name,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                num_labels=num_labels,\n",
    "                output_dir=f\"./results/scheme{scheme_id}_{exp_name}\",\n",
    "                learning_rate=config.get('learning_rate', 2e-5),\n",
    "                batch_size=config.get('batch_size', 16),\n",
    "                num_epochs=config.get('num_epochs', 3),\n",
    "                use_lora=use_lora,\n",
    "                lora_config=lora_config\n",
    "            )\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results.append({\n",
    "                'scheme': scheme_id,\n",
    "                'model': model_name,\n",
    "                'config': exp_name,\n",
    "                'lora': 'Yes' if use_lora else 'No',\n",
    "                'f1_macro': eval_results['eval_f1'],\n",
    "                'precision': eval_results['eval_precision'],\n",
    "                'recall': eval_results['eval_recall'],\n",
    "                'accuracy': eval_results['eval_accuracy']\n",
    "            })\n",
    "            \n",
    "            # Guardar el trainer para usar en predicciones\n",
    "            trainers[exp_name] = (trainer, tokenizer)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en el experimento {exp_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\nResultados de los experimentos:\")\n",
    "    display(results_df)\n",
    "    \n",
    "    return results_df, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para realizar y evaluar predicciones\n",
    "def predict_and_evaluate(trainer, X_test, y_test, tokenizer, label_encoder, model_name):\n",
    "    \"\"\"\n",
    "    Realiza predicciones y evalúa el rendimiento\n",
    "    \n",
    "    Args:\n",
    "        trainer: Modelo entrenado\n",
    "        X_test, y_test: Datos de prueba\n",
    "        tokenizer: Tokenizador para los datos\n",
    "        label_encoder: Codificador de etiquetas\n",
    "        model_name: Nombre del modelo para reportes\n",
    "        \n",
    "    Returns:\n",
    "        report: Informe de clasificación\n",
    "        predictions: Etiquetas predichas\n",
    "    \"\"\"\n",
    "    # Preparar dataset de prueba\n",
    "    test_dataset = prepare_test_dataset(X_test, tokenizer)\n",
    "    \n",
    "    # Realizar predicciones\n",
    "    print(f\"\\nGenerando predicciones con modelo {model_name}...\")\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_classes = predictions.predictions.argmax(-1)\n",
    "    \n",
    "    # Convertir predicciones a etiquetas\n",
    "    pred_labels = label_encoder.inverse_transform(pred_classes)\n",
    "    \n",
    "    # Si tenemos etiquetas para el test, evaluar rendimiento\n",
    "    if y_test is not None:\n",
    "        # Revertir la codificación de etiquetas para el informe\n",
    "        true_labels = label_encoder.inverse_transform(y_test)\n",
    "        \n",
    "        # Generar informe de clasificación\n",
    "        report = classification_report(true_labels, pred_labels, output_dict=True)\n",
    "        report_str = classification_report(true_labels, pred_labels)\n",
    "        print(\"\\nInforme de clasificación:\")\n",
    "        print(report_str)\n",
    "        \n",
    "        # Generar matriz de confusión\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm = confusion_matrix(y_test, pred_classes)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Matriz de Confusión - {model_name}\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Generar matriz de confusión normalizada\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm_norm = confusion_matrix(y_test, pred_classes, normalize='true')\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=label_encoder.classes_)\n",
    "        disp.plot(cmap=\"Blues\", values_format=\".2f\")\n",
    "        plt.title(f\"Matriz de Confusión Normalizada - {model_name}\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return report, pred_labels\n",
    "    else:\n",
    "        print(\"No hay etiquetas disponibles para evaluar el rendimiento en el conjunto de prueba\")\n",
    "        return None, pred_labels\n",
    "\n",
    "# Función para evaluar predicciones con todas las instancias de dev_gold\n",
    "def evaluate_on_complete_dev_gold(trainer, tokenizer, label_encoder, model_name):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en el conjunto completo de dev_gold\n",
    "    \n",
    "    Args:\n",
    "        trainer: Modelo entrenado\n",
    "        tokenizer: Tokenizador para los datos\n",
    "        label_encoder: Codificador de etiquetas\n",
    "        model_name: Nombre del modelo para reportes\n",
    "    \n",
    "    Returns:\n",
    "        report: Informe de clasificación\n",
    "    \"\"\"\n",
    "    if dev_gold_data is None or dev_data is None:\n",
    "        print(\"No se pueden realizar predicciones en dev_gold: datos no disponibles\")\n",
    "        return None\n",
    "    \n",
    "    X_complete = dev_data[\"Teaser Text\"].tolist()\n",
    "    y_complete = dev_gold_data[\"Tag Value\"].tolist()\n",
    "    \n",
    "    # Codificar etiquetas con el mismo codificador\n",
    "    y_complete_encoded = label_encoder.transform(y_complete)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluando modelo {model_name} en el conjunto COMPLETO de dev_gold ({len(X_complete)} instancias)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    report, _ = predict_and_evaluate(\n",
    "        trainer=trainer,\n",
    "        X_test=X_complete,\n",
    "        y_test=y_complete_encoded,\n",
    "        tokenizer=tokenizer,\n",
    "        label_encoder=label_encoder,\n",
    "        model_name=f\"{model_name} (Dev Gold Completo)\"\n",
    "    )\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edcf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir configuraciones de modelos a utilizar\n",
    "model_configs = [\n",
    "    # BERT base sin LoRA\n",
    "    {\n",
    "        'model_name': \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "        'name': 'base',\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 3,\n",
    "        'use_lora': False\n",
    "    },\n",
    "    # BERT base con LoRA\n",
    "    {\n",
    "        'model_name': \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "        'name': 'base',\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 3,\n",
    "        'use_lora': True,\n",
    "        'lora_params': {\n",
    "            'r': 8,\n",
    "            'lora_alpha': 16,\n",
    "            'lora_dropout': 0.1,\n",
    "            'target_modules': [\"query\", \"key\", \"value\"]\n",
    "        }\n",
    "    },\n",
    "    # RoBERTa base sin LoRA\n",
    "    {\n",
    "        'model_name': \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "        'name': 'base',\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 3,\n",
    "        'use_lora': False\n",
    "    },\n",
    "    # RoBERTa base con LoRA\n",
    "    {\n",
    "        'model_name': \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "        'name': 'base',\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 3,\n",
    "        'use_lora': True,\n",
    "        'lora_params': {\n",
    "            'r': 8,\n",
    "            'lora_alpha': 16,\n",
    "            'lora_dropout': 0.1,\n",
    "            'target_modules': [\"query\", \"key\", \"value\"]\n",
    "        }\n",
    "    },\n",
    "    # RoBERTa con LoRA optimizado\n",
    "    {\n",
    "        'model_name': \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "        'name': 'optimized',\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 5,\n",
    "        'use_lora': True,\n",
    "        'lora_params': {\n",
    "            'r': 16,  # Mayor rango para más capacidad\n",
    "            'lora_alpha': 32,\n",
    "            'lora_dropout': 0.2,\n",
    "            'target_modules': [\"query\", \"key\", \"value\", \"dense\"]  # Incluir capas densas\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar experimentos para cada esquema\n",
    "all_results = []\n",
    "all_reports = {}\n",
    "dev_gold_complete_reports = {}\n",
    "\n",
    "for scheme_id in [1, 2, 3]:\n",
    "    print(f\"\\n\\n{'#'*80}\")\n",
    "    print(f\"# EJECUTANDO EXPERIMENTOS PARA ESQUEMA {scheme_id}\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    # Obtener datos para este esquema\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, label_mapping, label_encoder = data_schemes[scheme_id]\n",
    "    \n",
    "    # Verificar que tenemos datos válidos\n",
    "    if X_train is None or y_train is None:\n",
    "        print(f\"No se pueden ejecutar experimentos para el esquema {scheme_id}: datos no disponibles\")\n",
    "        continue\n",
    "    \n",
    "    # Ejecutar experimentos\n",
    "    results_df, trainers = run_experiments(\n",
    "        scheme_id=scheme_id,\n",
    "        X_train=X_train, \n",
    "        y_train=y_train, \n",
    "        X_val=X_val, \n",
    "        y_val=y_val,\n",
    "        label_mapping=label_mapping,\n",
    "        model_configs=model_configs\n",
    "    )\n",
    "    \n",
    "    # Añadir resultados a la lista global\n",
    "    all_results.append(results_df)\n",
    "    \n",
    "    # Para cada modelo entrenado, generar predicciones y evaluación\n",
    "    for model_name, (trainer, tokenizer) in trainers.items():\n",
    "        # Primero evaluamos en el conjunto de prueba del esquema actual\n",
    "        if X_test is not None:\n",
    "            report, _ = predict_and_evaluate(\n",
    "                trainer=trainer,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                tokenizer=tokenizer,\n",
    "                label_encoder=label_encoder,\n",
    "                model_name=f\"Esquema {scheme_id} - {model_name}\"\n",
    "            )\n",
    "            \n",
    "            if report:\n",
    "                all_reports[f\"Esquema {scheme_id} - {model_name}\"] = report\n",
    "        \n",
    "        # Luego evaluamos en el conjunto completo de dev_gold\n",
    "        if has_gold:\n",
    "            dev_gold_report = evaluate_on_complete_dev_gold(\n",
    "                trainer=trainer,\n",
    "                tokenizer=tokenizer,\n",
    "                label_encoder=label_encoder,\n",
    "                model_name=f\"Esquema {scheme_id} - {model_name}\"\n",
    "            )\n",
    "            \n",
    "            if dev_gold_report:\n",
    "                dev_gold_complete_reports[f\"Esquema {scheme_id} - {model_name}\"] = dev_gold_report\n",
    "\n",
    "# Combinar todos los resultados\n",
    "if all_results:\n",
    "    combined_results = pd.concat(all_results)\n",
    "    combined_results = combined_results.sort_values(by='f1_macro', ascending=False)\n",
    "\n",
    "    print(\"\\n\\nRESUMEN DE TODOS LOS EXPERIMENTOS (ordenados por F1-macro):\")\n",
    "    display(combined_results)\n",
    "\n",
    "    # Guardar resultados\n",
    "    combined_results.to_csv(\"experimentos_clickbait_resultados_completos.csv\", index=False)\n",
    "\n",
    "    # Identificar el mejor modelo\n",
    "    if not combined_results.empty:\n",
    "        best_model = combined_results.iloc[0]\n",
    "        print(f\"\\nMEJOR MODELO: {best_model['config']} (Esquema {best_model['scheme']})\")\n",
    "        print(f\"F1-macro: {best_model['f1_macro']:.4f}\")\n",
    "        print(f\"Precisión: {best_model['precision']:.4f}\")\n",
    "        print(f\"Recall: {best_model['recall']:.4f}\")\n",
    "        print(f\"Exactitud: {best_model['accuracy']:.4f}\")\n",
    "        \n",
    "        # Crear un archivo con el reporte completo del mejor modelo en dev_gold\n",
    "        best_model_key = f\"Esquema {best_model['scheme']} - {best_model['config']}\"\n",
    "        if best_model_key in dev_gold_complete_reports:\n",
    "            best_report = dev_gold_complete_reports[best_model_key]\n",
    "            \n",
    "            # Convertir el informe a DataFrame para mejor visualización\n",
    "            report_df = pd.DataFrame(best_report).T\n",
    "            report_df = report_df.drop('support', axis=1, errors='ignore')\n",
    "            \n",
    "            print(\"\\nINFORME DE CLASIFICACIÓN DEL MEJOR MODELO EN DEV_GOLD COMPLETO:\")\n",
    "            display(report_df)\n",
    "            \n",
    "            # Guardar informe\n",
    "            report_df.to_csv(\"mejor_modelo_reporte_dev_gold.csv\")\n",
    "else:\n",
    "    print(\"\\n\\nNo se pudieron obtener resultados de ningún experimento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a33e3",
   "metadata": {},
   "source": [
    "## 8. Análisis Comparativo de Resultados\n",
    "\n",
    "En esta sección realizamos un análisis detallado de los diferentes experimentos y sus resultados.\n",
    "\n",
    "### Comparación de Esquemas de Entrenamiento\n",
    "\n",
    "Hemos evaluado tres esquemas diferentes de división de datos:\n",
    "\n",
    "1. **Esquema 1**: Uso de 75% del conjunto train para entrenamiento y 25% para validación, con evaluación final en dev_gold.\n",
    "2. **Esquema 2**: Uso del 100% del conjunto train para entrenamiento y evaluación en todo el conjunto dev_gold.\n",
    "3. **Esquema 3**: Uso del 100% del conjunto train para entrenamiento y división del conjunto dev_gold (50% validación, 50% prueba).\n",
    "\n",
    "### Comparación de Modelos\n",
    "\n",
    "Hemos comparado dos arquitecturas de modelos:\n",
    "\n",
    "- **BERT** (dccuchile/bert-base-spanish-wwm-cased): Modelo base en español con tokenización sensible a mayúsculas/minúsculas.\n",
    "- **RoBERTa** (PlanTL-GOB-ES/roberta-base-bne): Versión mejorada de BERT entrenada con el corpus de la Biblioteca Nacional de España.\n",
    "\n",
    "### Impacto de LoRA\n",
    "\n",
    "Hemos evaluado la efectividad de la técnica Low-Rank Adaptation (LoRA) para ajuste fino con diferentes configuraciones:\n",
    "- LoRA básico: r=8, alpha=16, dropout=0.1\n",
    "- LoRA optimizado: r=16, alpha=32, dropout=0.2, incluyendo capas densas\n",
    "\n",
    "### Análisis por Clase\n",
    "\n",
    "Analizamos el rendimiento específico por clase, con especial atención en:\n",
    "- Precisión, recall y F1-score para la clase \"Clickbait\"\n",
    "- Principales confusiones en la matriz de confusión\n",
    "- Casos donde el modelo presenta mayor dificultad\n",
    "\n",
    "### Conclusiones Principales\n",
    "\n",
    "[Este espacio se completará automáticamente con los resultados de los experimentos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47734c6",
   "metadata": {},
   "source": [
    "## 9. Recomendaciones y Trabajo Futuro\n",
    "\n",
    "Basándonos en los resultados obtenidos, podemos hacer las siguientes recomendaciones:\n",
    "\n",
    "### Configuración Óptima\n",
    "\n",
    "- **Modelo Recomendado**: [se completará con el mejor modelo]\n",
    "- **Esquema de Entrenamiento**: [se completará con el mejor esquema]\n",
    "- **Uso de LoRA**: [recomendación basada en resultados]\n",
    "\n",
    "### Áreas de Mejora\n",
    "\n",
    "1. **Balanceo de Clases**: Nuestros experimentos muestran que [observaciones sobre el desbalance]\n",
    "\n",
    "2. **Ajuste de Hiperparámetros**: [recomendaciones específicas]\n",
    "\n",
    "3. **Arquitectura del Modelo**: [observaciones sobre BERT vs RoBERTa]\n",
    "\n",
    "### Trabajo Futuro\n",
    "\n",
    "1. **Explorar modelos más grandes**: Modelos como RoBERTa-large o BETO podrían ofrecer mejores resultados.\n",
    "\n",
    "2. **Técnicas avanzadas de regularización**: Para evitar el sobreajuste, se podrían explorar más técnicas.\n",
    "\n",
    "3. **Análisis cualitativo de errores**: Un análisis detallado de los tweets mal clasificados podría proporcionar insights valiosos.\n",
    "\n",
    "4. **Ensembles de modelos**: Combinar las predicciones de varios modelos podría mejorar la robustez del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Calcula métricas de evaluación para las predicciones del modelo.\n",
    "    \n",
    "    Args:\n",
    "        pred: Objeto de predicción de Hugging Face\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con métricas calculadas\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad672d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para tokenizar y preparar datasets\n",
    "def prepare_datasets(X_train, y_train, X_val, y_val, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokeniza textos y prepara datasets de PyTorch\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Datos de entrenamiento\n",
    "        X_val, y_val: Datos de validación\n",
    "        tokenizer: Tokenizador de transformers\n",
    "        max_length: Longitud máxima de secuencia\n",
    "        \n",
    "    Returns:\n",
    "        train_dataset, val_dataset: Datasets listos para entrenamiento\n",
    "    \"\"\"\n",
    "    # Tokenizar textos\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    # Crear clase Dataset personalizada\n",
    "    class ClickbaitDataset(Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels is not None:\n",
    "                item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "    \n",
    "    # Crear dataset de entrenamiento\n",
    "    train_dataset = ClickbaitDataset(train_encodings, y_train)\n",
    "    \n",
    "    # Crear dataset de validación si hay etiquetas\n",
    "    if X_val is not None:\n",
    "        val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=max_length)\n",
    "        if y_val is not None:\n",
    "            val_dataset = ClickbaitDataset(val_encodings, y_val)\n",
    "        else:\n",
    "            val_dataset = ClickbaitDataset(val_encodings)\n",
    "    else:\n",
    "        val_dataset = None\n",
    "        \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Función para tokenizar datos de prueba\n",
    "def prepare_test_dataset(X_test, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokeniza textos de prueba y prepara dataset\n",
    "    \n",
    "    Args:\n",
    "        X_test: Datos de prueba\n",
    "        tokenizer: Tokenizador de transformers\n",
    "        max_length: Longitud máxima de secuencia\n",
    "        \n",
    "    Returns:\n",
    "        test_dataset: Dataset listo para inferencia\n",
    "    \"\"\"\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "    \n",
    "    test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
    "    return TestDataset(test_encodings)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
